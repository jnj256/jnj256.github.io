---
---

@string{aps = {American Physical Society,}}

@inproceedings{joshi2024factorizephys,
  title={FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing},
  author={Jitesh Joshi and Sos Agaian and Youngjun Cho},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=qrfp4eeZ47},
  abstract = {Remote photoplethysmography (rPPG) enables non-invasive extraction of blood volume pulse signals through imaging, transforming spatial-temporal data into time series signals. Advances in end-to-end rPPG approaches have focused on this transformation where attention mechanisms are crucial for feature extraction. However, existing methods compute attention disjointly across spatial, temporal, and channel dimensions. Here, we propose the Factorized Self-Attention Module (FSAM), which jointly computes multidimensional attention from voxel embeddings using nonnegative matrix factorization. To demonstrate FSAM's effectiveness, we developed FactorizePhys, an end-to-end 3D-CNN architecture for estimating blood volume pulse signals from raw video frames. Our approach adeptly factorizes voxel embeddings to achieve comprehensive spatial, temporal, and channel attention, enhancing performance of generic signal extraction tasks. Furthermore, we deploy FSAM within an existing 2D-CNN-based rPPG architecture to illustrate its versatility. FSAM and FactorizePhys are thoroughly evaluated against state-of-the-art rPPG methods, each representing different types of architecture and attention mechanism. We perform ablation studies to investigate the architectural decisions and hyperparameters of FSAM. Experiments on four publicly available datasets and intuitive visualization of learned spatial-temporal features substantiate the effectiveness of FSAM and enhanced cross-dataset generalization in estimating rPPG signals, suggesting its broader potential as a multidimensional attention mechanism. The code is accessible at https://github.com/PhysiologicAILab/FactorizePhys.},
  selected  = {true},
  preview = {FSAM.png}  
}


@article{joshi2024ibvp,
    title={iBVP Dataset: RGB-Thermal rPPG Dataset with High Resolution Signal Quality Labels},
    author={Joshi, Jitesh and Cho, Youngjun},
    journal={Electronics},
    publisher={MDPI},
    volume={13},
    year={2024},
    number={7},
    pages={1334},
    url={https://www.mdpi.com/2079-9292/13/7/1334},
    issn={2079-9292},
    abstract = {Remote photo-plethysmography (rPPG) has emerged as a non-intrusive and promising physiological sensing capability in human–computer interface (HCI) research, gradually extending its applications in health-monitoring and clinical care contexts. With advanced machine learning models, recent datasets collected in real-world conditions have gradually enhanced the performance of rPPG methods in recovering heart-rate and heart-rate-variability metrics. However, the signal quality of reference ground-truth PPG data in existing datasets is by and large neglected, while poor-quality references negatively influence models. Here, this work introduces a new imaging blood volume pulse (iBVP) dataset of synchronized RGB and thermal infrared videos with ground-truth PPG signals from ear with their high-resolution-signal-quality labels, for the first time. Participants perform rhythmic breathing, head-movement, and stress-inducing tasks, which help reflect real-world variations in psycho-physiological states. This work conducts dense (per sample) signal-quality assessment to discard noisy segments of ground-truth and corresponding video frames. We further present a novel end-to-end machine learning framework, iBVPNet, that features an efficient and effective spatio-temporal feature aggregation for the reliable estimation of BVP signals. Finally, this work examines the feasibility of extracting BVP signals from thermal video frames, which is under-explored. The iBVP dataset and source codes are publicly available for research use.},
    doi = {10.3390/electronics13071334},
    selected  = {true},
    preview = {iBVP_dataset.png}
}



@article{joshi2023physiokit,
  title={PhysioKit: An Open-Source, Low-Cost Physiological Computing Toolkit for Single-and Multi-User Studies},
  author={Joshi, Jitesh and Wang, Katherine and Cho, Youngjun},
  journal={Sensors},
  volume={23},
  number={19},
  pages={8244},
  year={2023},
  publisher={MDPI},
  url = {https://www.mdpi.com/1424-8220/23/19/8244},
  abstract = {The proliferation of physiological sensors opens new opportunities to explore interactions, conduct experiments and evaluate the user experience with continuous monitoring of bodily functions. Commercial devices, however, can be costly or limit access to raw waveform data, while low-cost sensors are efforts-intensive to setup. To address these challenges, we introduce PhysioKit, an open-source, low-cost physiological computing toolkit. PhysioKit provides a one-stop pipeline consisting of (i) a sensing and data acquisition layer that can be configured in a modular manner per research needs, and (ii) a software application layer that enables data acquisition, real-time visualization and machine learning (ML)-enabled signal quality assessment. This also supports basic visual biofeedback configurations and synchronized acquisition for co-located or remote multi-user settings. In a validation study with 16 participants, PhysioKit shows strong agreement with research-grade sensors on measuring heart rate and heart rate variability metrics data. Furthermore, we report usability survey results from 10 small-project teams (44 individual members in total) who used PhysioKit for 4–6 weeks, providing insights into its use cases and research benefits. Lastly, we discuss the extensibility and potential impact of the toolkit on the research community.},
  selected  = {true},
  preview = {physiokit.png}
}

@inproceedings{Joshi_2022_BMVC,
author    = {Joshi, Jitesh and Bianchi-Berthouze, Nadia and Cho, Youngjun},
title     = {Self-adversarial Multi-scale Contrastive Learning for Semantic Segmentation of Thermal Facial Images},
booktitle = {33rd British Machine Vision Conference 2022, {BMVC} 2022, London, UK, November 21-24, 2022},
publisher = {{BMVA} Press},
year      = {2022},
url       = {https://bmvc2022.mpi-inf.mpg.de/0864.pdf},
abstract  = {Segmentation of thermal facial images is a challenging task. This is because facial features often lack salience due to high-dynamic thermal range scenes and occlusion issues. Limited availability of datasets from unconstrained settings further limits the use of the state-of-the-art segmentation networks, loss functions and learning strategies which have been built and validated for RGB images. To address the challenge, we propose Self-Adversarial Multi-scale Contrastive Learning (SAM-CL framework as a new training strategy for thermal image segmentation. SAM-CL framework consists of a SAM-CL loss function and a thermal image augmentation (TiAug) module as a domain-specific augmentation technique. We use the Thermal-Face-Database to demonstrate effectiveness of our approach. Experiments conducted on the existing segmentation networks (UNET, Attention-UNET, DeepLabV3 and HRNetv2) evidence the consistent performance gains from the SAM-CL framework. Furthermore, we present a qualitative analysis with UBComfort and DeepBreath datasets to discuss how our proposed methods perform in handling unconstrained situations.},
selected  = {true},
preview   = {samcl.png}
}

@article{patent3Mml1,
 title={Detecting a condition for a culture device using a machine learning model},
 author={Tran, Thanh and Watson, Hugh and Joshi, Jitesh and SK, Abhilash and Tiwari, Rohitkumar},
 year={2021},
 journal={WIPO Patent WO2021234514A1},
 url={https://patents.google.com/patent/WO2021234514A1}
}

@article{patent3Mml2,
 title={Compensation of intensity variances in images used for colony enumeration},
 author={Tran, Thanh and Watson, Hugh and Joshi, Jitesh and Patel, Rinkeshkumar},
 year={2021},
 journal={WIPO Patent WO2021229337A1},
 url={https://patents.google.com/patent/WO2021229337A1}
 }

@article{patent3Moptics1,
 title={Imaging device with illumination components},
 author={Tran, Thanh and Watson, Hugh and Joshi, Jitesh},
 year={2021},
 journal={WIPO Patent WO2021229347A1},
 url={https://patents.google.com/patent/WO2021229347A1}
 }

@article{joshi2014boldsync,
  title={BOLDSync: A MATLAB-based toolbox for synchronized stimulus presentation in functional MRI},
  author={Joshi, Jitesh and Saharan, Sumiti and Mandal, Pravat K},
  journal={Journal of neuroscience methods},
  volume={223},
  pages={123--132},
  year={2014},
  publisher={Elsevier},
  preview = {BOLDSync.png}
}

@article{mandal2012visuospatial,
  title={Visuospatial perception: an emerging biomarker for Alzheimer's disease},
  author={Mandal, Pravat K and Joshi, Jitesh and Saharan, Sumiti},
  journal={Journal of Alzheimer's Disease},
  volume={31},
  number={s3},
  pages={S117--S135},
  year={2012},
  publisher={IOS Press},
  preview={visuospatial_perception.png}
}
